{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful imports and extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset and extracting climate change related videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to study youtube videos related to climate change : the first step is to filter our broad dataset of all videos.\n",
    "For this, we generated a list of keywords related to climate change :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'climate change', 'global warming', 'greenhouse gases', 'rising sea levels',\n",
    "    'extreme weather', 'temperature increase', 'climate action', \n",
    "    'carbon emissions', 'renewable energy', 'sustainable living',\n",
    "    'climate science', 'climate crisis', 'climate adaptation',\n",
    "    'natural disaster', 'extreme heat', 'melting ice cap', 'sea level rise',\n",
    "    'biodiversity loss', 'deforestation', 'ocean acidification', 'climate policy', \n",
    "    'environmental policy',\n",
    "    'disaster recovery', 'climate refugees',\n",
    "    'carbon footprint', 'sustainable development', 'green technology',\n",
    "    'renewable resource', 'eco-friendly',\n",
    "    'ecosystem disruption', 'impact on climate',\n",
    "    'paris agreement', 'climate awareness',\n",
    "    'environmental justice', 'clean energy', 'zero carbon',\n",
    "    'green infrastructure', 'ozone layer',\n",
    "    'pollution', 'water scarcity', 'climate education'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create functions to check if a video contains climate change related keywords in its title, description or tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def about_climate(tags):\n",
    "    tags_set = set(tags)\n",
    "    return any(tag.strip().lower() in keywords for tag in tags_set)\n",
    "\n",
    "def climate_text(text):\n",
    "    lowercase_text = text.lower()\n",
    "    return any(keyword in lowercase_text for keyword in keywords)\n",
    "\n",
    "def climate_related(video):\n",
    "    return (climate_text(video['title']) or about_climate(video['tags']) or climate_text(video['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original video metadata dataset is huge (97 GB), we read and filter the dataset using pyskark in order to get a reduced dataset with only the relevant videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_disk = '/Volumes/Maxtor'\n",
    "file_path = '/yt_metadata_en.jsonl'\n",
    "\n",
    "sc = SparkContext()\n",
    "raw_data = sc.textFile(path_to_disk + file_path)\n",
    "video_dataset = raw_data.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_videos = video_dataset.filter(climate_related)\n",
    "climate_videos_collected = climate_videos.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_videos_path = \"data/coarse_filter_climate_videos.jsonl\"\n",
    "with jsonlines.open(climate_videos_path, \"w\") as jsonl_file:\n",
    "   jsonl_file.write_all(climate_videos_collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create 2 random sample dataset (1% ands 0.1%) which may be useful later in our analysis for statisitical comparison with the cliamte related videos dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random_sample_1percent = video_dataset.sample(False, 0.01, seed=seed)\n",
    "random_sample_1percent_collected = random_sample_1percent.collect()\n",
    "sample_1percent_path = \"data/sample_1percent.jsonl\"\n",
    "with jsonlines.open(sample_1percent_path, \"w\") as jsonl_file:\n",
    "    jsonl_file.write_all(random_sample_1percent_collected)\n",
    "\n",
    "\n",
    "random_sample_01percent = video_dataset.sample(False, 0.001, seed=seed)\n",
    "random_sample_01percent_collected = random_sample_01percent.collect()\n",
    "sample_01percent_path = \"data/sample_01percent.jsonl\"\n",
    "with jsonlines.open(sample_01percent_path, \"w\") as jsonl_file:\n",
    "    jsonl_file.write_all(random_sample_01percent_collected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting useful features from the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep working on the whole dataset in order to extract useful features which may be useful later for comparison purpose. We use the .feather helper for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_path = \"data/yt_metadata_helper.feather\"\n",
    "video_dataset_feather = pd.read_feather(feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_count_by_date_cat = video_dataset_feather.groupby(['upload_date', 'categories']).size().reset_index(name='count')\n",
    "video_count_by_date = video_count_by_date_cat[['upload_date', 'count']].groupby(['upload_date']).sum().reset_index()\n",
    "video_count_by_cat = video_count_by_date_cat[['categories', 'count']].groupby(['categories']).sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_count_by_date_cat.to_json(\"data/nb_videos_by_date_cat.jsonl\", orient=\"records\", lines=True)\n",
    "video_count_by_date.to_json(\"data/nb_videos_by_date.jsonl\", orient=\"records\", lines=True)\n",
    "video_count_by_cat.to_json(\"data/nb_videos_by_cat.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
