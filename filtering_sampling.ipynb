{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTERING VIDEOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    'climate change', 'global warming', 'greenhouse gases', 'rising sea levels',\n",
    "    'extreme weather', 'temperature increase', 'climate action', \n",
    "    'carbon emissions', 'renewable energy', 'sustainable living',\n",
    "    'climate science', 'climate crisis', 'climate adaptation',\n",
    "    'natural disaster', 'extreme heat', 'melting ice cap', 'sea level rise',\n",
    "    'biodiversity loss', 'deforestation', 'ocean acidification', 'climate policy', \n",
    "    'environmental policy',\n",
    "    'disaster recovery', 'climate refugees',\n",
    "    'carbon footprint', 'sustainable development', 'green technology',\n",
    "    'renewable resource', 'eco-friendly',\n",
    "    'ecosystem disruption', 'impact on climate',\n",
    "    'paris agreement', 'climate awareness',\n",
    "    'environmental justice', 'clean energy', 'zero carbon',\n",
    "    'green infrastructure', 'ozone layer',\n",
    "    'pollution', 'water scarcity', 'climate education'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if text or tags contain climate keywords:\n",
    "#def idiot_filter(video):\n",
    "#    return video['upload_date'] > '2015'\n",
    "\n",
    "\n",
    "def about_climate(tags):\n",
    "    tags_set = set(tags)\n",
    "    return any(tag.strip().lower() in keywords for tag in tags_set)\n",
    "\n",
    "def climate_text(text):\n",
    "    lowercase_text = text.lower()\n",
    "    return any(keyword in lowercase_text for keyword in keywords)\n",
    "\n",
    "def climate_related(video):\n",
    "    return (climate_text(video['title']) or about_climate(video['tags']) or climate_text(video['description']))\n",
    "    #return idiot_filter(video) and (climate_text(video['title']) or about_climate(video['tags']) or climate_text(video['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We read and filter the video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/14 16:47:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Volumes/Maxtor/yt_metadata_en.jsonl'\n",
    "\n",
    "raw_data = sc.textFile(file_path)\n",
    "video_dataset = raw_data.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recent_videos = video_dataset.filter(idiot_filter)\n",
    "#climate_videos = recent_videos.filter(climate_related)\n",
    "climate_videos = video_dataset.filter(climate_related)\n",
    "#climate_videos_collected = climate_videos.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#climate_videos_path = \"data/climate_videos_new.jsonl\"\n",
    "\n",
    "#with jsonlines.open(climate_videos_path, \"w\") as jsonl_file:\n",
    "#    jsonl_file.write_all(climate_videos_collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we count the number of video uploaded each day\n",
    "\n",
    "This may be useful for our future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb_videos_by_date = video_dataset.map(lambda x: (x[\"upload_date\"].split()[0], 1))\n",
    "\n",
    "# Group by date and count occurrences\n",
    "nb_videos_by_date = nb_videos_by_date.groupBy(lambda x: x[0]).map(lambda x: (x[0], len(x[1])))\n",
    "\n",
    "# Collect the result\n",
    "nb_videos_by_date_collected = nb_videos_by_date.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_by_date_path = \"data/nb_videos_by_date.jsonl\"\n",
    "\n",
    "with jsonlines.open(videos_by_date_path, \"w\") as jsonl_file:\n",
    "    jsonl_file.write_all(nb_videos_by_date_collected)\n",
    "\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
